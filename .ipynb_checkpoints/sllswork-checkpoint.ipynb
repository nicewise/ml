{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a370b11",
   "metadata": {},
   "source": [
    "# 处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a297af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45f7e63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seri(x, s):\n",
    "    h,w = x.shape\n",
    "    o = np.zeros((h-s+1)*s*w).reshape(h-s+1, s, w)\n",
    "    for i in range(h-s+1):\n",
    "        o[i] = x[i:i+s]\n",
    "    return(o)\n",
    "def xy(data_t, s):\n",
    "    data_y = data[sql:, :3]\n",
    "    data_x = np.concatenate((data[:-1, :3], data[1:, 3:]), axis=1)\n",
    "    data_x = seri(data_x, s)\n",
    "    return(data_x, data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0372b7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class myDataset(Dataset):\n",
    "    def __init__(self,x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04909808",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(x):\n",
    "    _, l = x.shape\n",
    "    x_min = []\n",
    "    x_space = []\n",
    "    for i in range(l):\n",
    "        x_min_i = x[:, i].min()\n",
    "        x_min.append(x_min_i)\n",
    "        x_space.append(x[:, i].max() - x_min_i)\n",
    "        x[:, i] = (x[:, i] - x_min_i) / x_space[i]\n",
    "    return(x, x_min, x_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c83b28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30874, 5)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"data4.csv\")\n",
    "data = np.array(data, dtype = np.float32)\n",
    "print(data.shape)\n",
    "data, x_min, x_space = normalization(data)\n",
    "sql = 50 # 序列长度\n",
    "data0 = data[0:2604]\n",
    "data5 = data[2604:6025]\n",
    "data10 = data[6025:10019]\n",
    "data15 = data[10019:14678]\n",
    "data25 = data[14678:20032]\n",
    "data30 = data[20032:25739]\n",
    "\n",
    "(x0, y0) = xy(data0, sql)\n",
    "(x5, y5) = xy(data5, sql)\n",
    "(x10, y10) = xy(data10, sql)\n",
    "(x15, y15) = xy(data15, sql)\n",
    "(x25, y25) = xy(data25, sql)\n",
    "(x30, y30) = xy(data30, sql)\n",
    "train_x = np.concatenate((x0, x5, x10, x15, x25, x30), axis=0)\n",
    "train_y = np.concatenate((y0, y5, y10, y15, y25, y30), axis=0)\n",
    "data20 = data[25739:30874]\n",
    "(test_x, test_y) = xy(data20, sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e81b8328",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = myDataset(train_x, train_y)\n",
    "test_dataset = myDataset(test_x, test_y)\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset = test_dataset, batch_size = batch_size)\n",
    "# for step, (batch_x, batch_y) in enumerate(train_loader):\n",
    "#     print('Step: ', step, '| batch x: ', batch_x.shape, '| batch y: ', batch_y.shape)\n",
    "#     model(batch_x.float())\n",
    "#     print(batch_x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f3c0ef",
   "metadata": {},
   "source": [
    "# 网络搭建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7a6f5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/site-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "class cons(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layer, output_size):\n",
    "        super(cons, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layer, dropout=0.2, batch_first=True)\n",
    "        # 上面这个dropout=0.2, 不会在最后一层之后加dropout层\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.dense = nn.Linear(hidden_size, output_size)\n",
    "    def forward(self, x):\n",
    "        l_out, (hn, cn) = self.lstm(x)\n",
    "        o = hn.squeeze(0)\n",
    "#         o = self.dropout(o)\n",
    "        o = self.dense(o)\n",
    "        o = F.relu(o)\n",
    "        return o\n",
    "\n",
    "input_size, hidden_size, num_layer, output_size = 5, 120, 1, 3\n",
    "model = cons(input_size, hidden_size, num_layer, output_size) # 实例化模型\n",
    "model.float()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a2009e",
   "metadata": {},
   "source": [
    "# GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97b7f64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68002734",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d9f2f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "# from datetime import datetime\n",
    "def train_one_epoch(epoch_index):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "    for i, data in enumerate(train_loader, 1):\n",
    "        inputs, actual = data\n",
    "        outputs = model(inputs.float())\n",
    "        loss = loss_fn(outputs, actual)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if (i+1) % 200 == 0:\n",
    "            last_loss = running_loss / 200 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            running_loss = 0.\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d631dafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 200 loss: 0.02702657060159254\n",
      "  batch 400 loss: 0.0004779542959659011\n",
      "  batch 600 loss: 0.0002519259861946921\n",
      "  batch 800 loss: 0.0003387968926836038\n",
      "  batch 1000 loss: 0.00034852411507017676\n",
      "  batch 1200 loss: 0.0003266298289599945\n",
      "  batch 1400 loss: 0.0002303705353551777\n",
      "  batch 1600 loss: 0.00017265298331949451\n",
      "  batch 1800 loss: 0.0001942735526517936\n",
      "  batch 2000 loss: 0.0002912315325193049\n",
      "  batch 2200 loss: 0.0002509161129091808\n",
      "  batch 2400 loss: 0.00022015360327714005\n",
      "  batch 2600 loss: 0.00012004677251752583\n",
      "  batch 2800 loss: 0.0001031552129143165\n",
      "LOSS: train 0.0001031552129143165 test 0.0001093172759283334\n",
      "EPOCH 2:\n",
      "  batch 200 loss: 0.00011438010352776473\n",
      "  batch 400 loss: 8.971787383416086e-05\n",
      "  batch 600 loss: 0.0002554857101995367\n",
      "  batch 800 loss: 4.394959923956776e-05\n",
      "  batch 1000 loss: 0.00013636595524531004\n",
      "  batch 1200 loss: 7.406699575312814e-05\n",
      "  batch 1400 loss: 6.961416242347696e-05\n",
      "  batch 1600 loss: 4.049344030136126e-05\n",
      "  batch 1800 loss: 0.0001166533702689776\n",
      "  batch 2000 loss: 7.338322662462815e-05\n",
      "  batch 2200 loss: 4.728340896917871e-05\n",
      "  batch 2400 loss: 8.250494462799907e-05\n",
      "  batch 2600 loss: 3.935718331717908e-05\n",
      "  batch 2800 loss: 5.001900194884001e-05\n",
      "LOSS: train 5.001900194884001e-05 test 4.450347478268668e-05\n",
      "EPOCH 3:\n",
      "  batch 200 loss: 3.6095541343001966e-05\n",
      "  batch 400 loss: 4.2836671088934966e-05\n",
      "  batch 600 loss: 3.178023752411718e-05\n",
      "  batch 800 loss: 2.2015236230572554e-05\n",
      "  batch 1000 loss: 7.637399961595293e-06\n",
      "  batch 1200 loss: 6.66047970321415e-05\n",
      "  batch 1400 loss: 3.4590153190947604e-05\n"
     ]
    }
   ],
   "source": [
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 500\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number)\n",
    "\n",
    "    model.train(False)\n",
    "#     model.eval()\n",
    "    running_tloss = 0.0\n",
    "    for i, tdata in enumerate(test_loader):\n",
    "        tinputs, tactual = tdata\n",
    "        toutputs = model(tinputs.float())\n",
    "        tloss = loss_fn(toutputs, tactual)\n",
    "        running_tloss += tloss\n",
    "\n",
    "    avg_tloss = running_tloss / (i + 1)\n",
    "    print('LOSS: train {} test {}'.format(avg_loss, avg_tloss))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    epoch_number += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
